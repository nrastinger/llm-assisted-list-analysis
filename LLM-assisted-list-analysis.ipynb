{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **LLM-based NER for historical lists: From semi-structured texts to structured data**\n"
      ],
      "metadata": {
        "id": "KBPqtn5xex_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<style>\n",
        ".full-width-banner {\n",
        "    position: relative;\n",
        "    left: 50%;\n",
        "    right: 50%;\n",
        "    margin-left: -50vw;\n",
        "    margin-right: -50vw;\n",
        "    width: 100vw;\n",
        "    height: 300px;\n",
        "    overflow: hidden;\n",
        "    z-index: 1;\n",
        "}\n",
        ".full-width-banner img {\n",
        "    width: 100%;\n",
        "    height: 100%;\n",
        "    object-fit: cover;\n",
        "    display: block;\n",
        "}\n",
        "</style>\n",
        "\n",
        "<div class=\"full-width-banner\">\n",
        "    <img src=\"https://raw.githubusercontent.com/nrastinger/llm-assisted-list-analysis/refs/heads/main/images/deco-banner-small.png\" alt=\"Banner\">\n",
        "</div>"
      ],
      "metadata": {
        "id": "LWd8Y93mRtYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Jupyter Notebook was initially created as part of a broader case study titled [\"Visiting Vienna\"](https://www.oeaw.ac.at/acdh/research/literary-textual-studies/research/linked-data-vienna-time-machine/visiting-vienna) that focused on **automatic information extraction from periodically published lists found in historical newspapers and advertising papers** (cf. [Rastinger 2024](https://doi.org/10.3384/ecp210016)). Specifically, it addressed the arrival lists published in the *Wien[n]erisches Diarium* (WD) from 1703 to 1725 and demonstrated how **Large Language Models (LLMs)** can be used for **Named Entity Recognition (NER)** to transform  historical semi-structured texts into structured data better suitable for analysis. With the idea of re-using this workflow for further (types of) lists, the Notebook has since been updated and further expanded. (Note: While the Notebook is optimized for seamless use in Google Colab — making it accessible without the need for extensive local setup — it can easily be adapted for local environments with minimal modifications.)"
      ],
      "metadata": {
        "id": "mA8sMZw88aKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Preparation**"
      ],
      "metadata": {
        "id": "UhAvBZkKXBeR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before use, the following preparations must be made:\n",
        "*   **Input data** (= full texts of single lists or list items): prepared as either a folder with txt-files or a csv-file\n",
        "*   **OpenAI API Acess**: OpenAI Account and API Key needed (stored in Colab Secrets as `\"openai_api\"`)\n",
        "*   **Prompt Template** (`\"prompt.jinja\"`) is needed be stored in same folder as the Jupyter notebook\n",
        "\n",
        "The easiest way to access the prompt template and exemplary data is to **clone the Github repository** by running the following cell (and then just leave the file paths as they are):"
      ],
      "metadata": {
        "id": "IaAQYJCT8Zmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nrastinger/llm-assisted-list-analysis\n",
        "%cd llm-assisted-list-analysis"
      ],
      "metadata": {
        "id": "xh_csVuFrJ1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up necessary libraries and modules"
      ],
      "metadata": {
        "id": "fRP0r31dDxGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell makes sure all necessary Python libraries and modules and functions are installed and/or imported (Note: suggested restart of Notebook afterwards is optional)."
      ],
      "metadata": {
        "id": "EUTPX86VWb3_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "x3Lglb60eeU-"
      },
      "outputs": [],
      "source": [
        "# Installation of needed libraries and modules\n",
        "\n",
        "!pip install promptify\n",
        "!pip install nltk\n",
        "!pip install seqeval\n",
        "from promptify import Prompter, OpenAI, Pipeline\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "from google.colab import userdata, files\n",
        "import json\n",
        "import textwrap\n",
        "from tabulate import tabulate\n",
        "from collections import Counter\n",
        "from itertools import count\n",
        "from IPython.display import display, HTML\n",
        "import xml.etree.ElementTree as ET\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from seqeval.metrics import accuracy_score\n",
        "from seqeval.metrics import classification_report\n",
        "from seqeval.metrics import f1_score\n",
        "from seqeval.metrics import precision_score\n",
        "from seqeval.metrics import recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining necessary functions"
      ],
      "metadata": {
        "id": "g7RiYQkQ_faU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell makes sure all necessary functions are defined."
      ],
      "metadata": {
        "id": "oXUFc3Rf_jtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions for input text cleaning\n",
        "\n",
        "def remove_empty_lines(text): #removes empty lines\n",
        "    lines = text.splitlines()\n",
        "    non_empty_lines = [line for line in lines if line.strip()]\n",
        "    cleaned_text = '\\n'.join(non_empty_lines)\n",
        "    return cleaned_text\n",
        "\n",
        "def clean(text, normalize_hyphens=\"no\"): #basic cleaning of full texts; double hyphens can be normalized (= > -) or kept; default: kept\n",
        "    if normalize_hyphens == \"yes\":\n",
        "      cleaned = text.replace(\"=\", \"-\")\n",
        "      cleaned0 = remove_empty_lines(cleaned)\n",
        "    else:\n",
        "      cleaned0 = remove_empty_lines(text)\n",
        "    cleaned_1 = re.sub(\"[=-]\\\\n(?=([a-z]|[üöäßéæ]))\", \"\", cleaned0)\n",
        "    cleaned_2 =  re.sub(\"[=-]\\\\n(?=[A-Z]|[ÄÖÜ])\", \"-\", cleaned_1)\n",
        "    cleaned_3 = re.sub(\"\\\\n\", \" \", cleaned_2)\n",
        "    cleaned_4 = re.sub(\" +\", \" \", cleaned_3)\n",
        "    return cleaned_4.strip()\n",
        "\n",
        "# Function for NER via Promptify\n",
        "\n",
        "def ner(text, labels=None, examples=[], description=None, domain=None, usage_statistics=False):\n",
        "    result = pipe.fit(text,\n",
        "                      domain=domain,\n",
        "                      examples=examples,\n",
        "                      labels=labels,\n",
        "                      description=description)\n",
        "\n",
        "    completion = result[0][\"parsed\"][\"data\"][\"completion\"]\n",
        "\n",
        "    # Normalize the structure in case of different output formats\n",
        "    if isinstance(completion, list):\n",
        "        if len(completion) > 0 and isinstance(completion[0], list):\n",
        "            # If it's a list of lists, extract the first inner list\n",
        "            normalized_completion = completion[0]\n",
        "        else:\n",
        "            # Already in the correct form\n",
        "            normalized_completion = completion\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected structure in completion data\")\n",
        "\n",
        "    # basic modus = only result\n",
        "    if usage_statistics == False:\n",
        "      return normalized_completion\n",
        "\n",
        "    # detailed = result plus usage statistics\n",
        "    elif usage_statistics == True:\n",
        "      prompt_tokens = result[0][\"usage\"][\"prompt_tokens\"]\n",
        "      completion_tokens = result[0][\"usage\"][\"completion_tokens\"]\n",
        "      total_tokens = result[0][\"usage\"][\"total_tokens\"]\n",
        "      return normalized_completion, prompt_tokens, completion_tokens, total_tokens\n",
        "\n",
        "# Setting of default parameters - to avoid errors\n",
        "\n",
        "labels=None\n",
        "examples=[]\n",
        "domain=None\n",
        "usage_statistics=False\n",
        "\n",
        "# Function for integrating entity IDs\n",
        "\n",
        "def add_unique_entity_ids(df, entity_col=\"entities\", id_key=\"id\"):\n",
        "    id_generator = count(1)\n",
        "\n",
        "    def assign_ids(entities):\n",
        "        if not isinstance(entities, list):\n",
        "            return entities\n",
        "        for entity in entities:\n",
        "            if isinstance(entity, dict):\n",
        "                entity[id_key] = f\"e{next(id_generator)}\"\n",
        "        return entities\n",
        "\n",
        "    df[entity_col] = df[entity_col].apply(assign_ids)\n",
        "    return df\n",
        "\n",
        "# Function for highlighting entities\n",
        "\n",
        "def highlight_entities(row):\n",
        "    text = row[\"text\"]\n",
        "    entities = row[\"entities\"]\n",
        "\n",
        "    if not isinstance(text, str) or not isinstance(entities, list):\n",
        "        return HTML(text)\n",
        "\n",
        "    # Sort and filter entities with positions\n",
        "    entities = [e for e in entities if isinstance(e, dict) and \"S\" in e and \"En\" in e]\n",
        "    entities.sort(key=lambda e: e[\"S\"])\n",
        "\n",
        "    result = \"\"\n",
        "    last_idx = 0\n",
        "\n",
        "    for entity in entities:\n",
        "        s, en = entity[\"S\"], entity[\"En\"]\n",
        "        etext = text[s:en]\n",
        "        etype = entity.get(\"T\", \"Other\")\n",
        "        eid = entity.get(\"id\", \"\")\n",
        "\n",
        "        color = label_to_color.get(etype, \"#dddddd\")  # fallback color\n",
        "\n",
        "        # Styled span for entity with mini pill for type & ID\n",
        "        result += text[last_idx:s]\n",
        "        result += (\n",
        "            f'<span style=\"background-color:{color}; color:black; padding:3px 6px; '\n",
        "            f'border-radius:16px; margin:0 2px; display:inline-block; '\n",
        "            f'box-shadow: 1px 1px 3px rgba(0,0,0,0.1);\">'\n",
        "            f'{etext}'\n",
        "            f'<span style=\"font-size:0.75em; font-weight:normal; margin-left:6px; '\n",
        "            f'background-color:rgba(0,0,0,0.05); padding:1px 6px; border-radius:12px;\">'\n",
        "            f'{etype}, {eid}</span></span>'\n",
        "        )\n",
        "        last_idx = en\n",
        "\n",
        "    result += text[last_idx:]\n",
        "    return result\n",
        "\n",
        "# Function for generating and exporting HTML file with displayed entities\n",
        "\n",
        "def save_highlighted_entities_to_html(df, filename=\"output.html\"):\n",
        "    # Start HTML document\n",
        "    html_parts = [\"<html><body>\"]\n",
        "\n",
        "    # Generate HTML for each row\n",
        "    for i, row in df.iterrows():\n",
        "        html = highlight_entities(row)  # must return raw HTML string\n",
        "        html_parts.append(f\"<div>{html}</div><hr style='margin:30px 0;'>\")\n",
        "\n",
        "    # End HTML document\n",
        "    html_parts.append(\"</body></html>\")\n",
        "\n",
        "    # Save to file\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(html_parts))\n",
        "\n",
        "    files.download(filename)\n",
        "\n",
        "# Functions for importing goldstandard from CATMA export folder\n",
        "\n",
        "def get_catma_annotations(base_folder, annotation_collection):\n",
        "    annotations = []\n",
        "    namespace = \"{http://www.tei-c.org/ns/1.0}\"  # TEI XML namespace\n",
        "    xml_id_attr = \"{http://www.w3.org/XML/1998/namespace}id\"  # Correct namespace for xml:id\n",
        "\n",
        "    for doc_folder in os.listdir(base_folder):\n",
        "        doc_path = os.path.join(base_folder, doc_folder)\n",
        "\n",
        "        # Ensure it's a directory (document subfolder)\n",
        "        if os.path.isdir(doc_path):\n",
        "            txt_file_path = None\n",
        "            xml_file_path = os.path.join(doc_path, \"annotationcollections\", annotation_collection + \".xml\")\n",
        "\n",
        "            # Locate the TXT file in the document folder\n",
        "            for file in os.listdir(doc_path):\n",
        "                if file.endswith(\".txt\"):\n",
        "                    txt_file_path = os.path.join(doc_path, file)\n",
        "                    break  # Stop after finding the first TXT file\n",
        "\n",
        "            # Ensure both the TXT and XML files exist\n",
        "            if txt_file_path and os.path.exists(xml_file_path):\n",
        "                with open(txt_file_path, 'r', encoding='utf-8') as txt_file:\n",
        "                    text_content = txt_file.read()\n",
        "\n",
        "                tree = ET.parse(xml_file_path)\n",
        "                root = tree.getroot()\n",
        "\n",
        "                # Mapping annotation IDs to type IDs from <fs> elements\n",
        "                fs_id_to_type = {}\n",
        "                for fs in root.findall(f\".//{namespace}fs\"):\n",
        "                    fs_id = fs.get(xml_id_attr)  # Using correct namespace for xml:id\n",
        "                    fs_type = fs.get(\"type\")\n",
        "                    if fs_id and fs_type:\n",
        "                        fs_id_to_type[fs_id] = fs_type\n",
        "\n",
        "                # Mapping type IDs to their readable labels from <fsDecl> elements\n",
        "                type_to_label = {}\n",
        "                for fs_decl in root.findall(f\".//{namespace}fsDecl\"):\n",
        "                    type_id = fs_decl.get(xml_id_attr)  # Using correct namespace for xml:id\n",
        "                    label_elem = fs_decl.find(f\"{namespace}fsDescr\")\n",
        "                    if type_id and label_elem is not None:\n",
        "                        type_to_label[type_id] = label_elem.text\n",
        "\n",
        "                # Extracting annotations\n",
        "                for seg in root.findall(f\".//{namespace}seg\"):\n",
        "                    ana = seg.get(\"ana\")\n",
        "                    ptr = seg.find(f\".//{namespace}ptr\")\n",
        "\n",
        "                    if ptr is not None and ana:\n",
        "                        ana = ana.lstrip(\"#\")  # Remove \"#\" prefix\n",
        "                        target = ptr.get(\"target\")\n",
        "\n",
        "                        if \"#char=\" in target:\n",
        "                            start, end = map(int, target.split(\"#char=\")[1].split(\",\"))\n",
        "                            segment_text = text_content[start:end]\n",
        "\n",
        "                            # Get label by first mapping to type, then to a readable label\n",
        "                            annotation_type = fs_id_to_type.get(ana, \"Unknown\")\n",
        "                            label = type_to_label.get(annotation_type, \"Unknown\")\n",
        "\n",
        "                            annotations.append({\n",
        "                                \"document\": doc_folder,  # Using subfolder name as document identifier\n",
        "                                \"annotation_id\": ana,\n",
        "                                \"label\": label,\n",
        "                                \"start\": start,\n",
        "                                \"end\": end,\n",
        "                                \"entity\": segment_text,\n",
        "                                \"text\": text_content\n",
        "                            })\n",
        "\n",
        "    df_catma = pd.DataFrame(annotations)\n",
        "    df_merged = merge_separated_annotations(df_catma)\n",
        "    df_merged['entity'] = df_merged.apply(lambda row: correct_entity_text(row, base_folder), axis=1)\n",
        "    df_merged['text'] = df_merged.apply(lambda row: correct_full_text(row, base_folder), axis=1)\n",
        "    return df_merged\n",
        "\n",
        "# Function for merging rows with the same annotation_id\n",
        "\n",
        "def merge_separated_annotations(df_catma):\n",
        "  merged_annotations = []\n",
        "  for annotation_id, group in df_catma.groupby(\"annotation_id\"):\n",
        "      min_start = group[\"start\"].min()\n",
        "      max_end = group[\"end\"].max()\n",
        "      first_row = group.iloc[0]  # Taking the first row for other attributes\n",
        "      merged_annotations.append({\n",
        "          \"document\": first_row[\"document\"],\n",
        "          \"annotation_id\": annotation_id,\n",
        "          \"label\": first_row[\"label\"],\n",
        "          \"start\": min_start,\n",
        "          \"end\": max_end,\n",
        "          \"entity\": first_row[\"entity\"], # Placeholder until correction\n",
        "          \"text\": first_row[\"text\"]\n",
        "      })\n",
        "\n",
        "  df_merged = pd.DataFrame(merged_annotations)\n",
        "  return df_merged\n",
        "\n",
        "#Correcting the 'entity' column after merging\n",
        "def correct_entity_text(row, base_folder):\n",
        "  doc = row['document']\n",
        "  start = row['start']\n",
        "  end = row['end']\n",
        "\n",
        "  # Finding the original text file\n",
        "  file_path = base_folder + '/' + doc + '/' + doc + '.txt'\n",
        "  with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "  return text[start:end]\n",
        "\n",
        "#Correcting the 'text' column after merging\n",
        "def correct_full_text(row, base_folder):\n",
        "  doc = row['document']\n",
        "\n",
        "  # Finding the original text file\n",
        "  file_path = base_folder + '/' + doc + '/' + doc + '.txt'\n",
        "  with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "  return text\n",
        "\n",
        "# Functions for identifying the correct positions of entities\n",
        "\n",
        "def add_positions_and_flag_ambiguities(row):\n",
        "    text = row[\"text\"]\n",
        "    entities = row[\"entities\"]\n",
        "\n",
        "    if not isinstance(text, str) or not isinstance(entities, list):\n",
        "        print(\"Invalid data types in:\", row)\n",
        "        return entities\n",
        "\n",
        "    used_spans = []\n",
        "    entities = copy.deepcopy(entities)\n",
        "    ent_texts = [e.get(\"E\") for e in entities if isinstance(e, dict)]\n",
        "    pred_counts = Counter(ent_texts)\n",
        "    actual_counts = {e: len(re.findall(rf'(?<![\\w\\-=\\u2011]){re.escape(e)}(?![\\w\\-=\\u2011])', text)) for e in set(ent_texts)}\n",
        "\n",
        "    for entity in entities:\n",
        "        if not isinstance(entity, dict):\n",
        "            print(\"Invalid data types in:\", row)\n",
        "            continue\n",
        "\n",
        "        etext = entity.get(\"E\")\n",
        "        if not isinstance(etext, str):\n",
        "            print(\"Invalid data types in:\", row)\n",
        "            continue\n",
        "\n",
        "        # Ambiguity check\n",
        "        if actual_counts.get(etext, 0) > pred_counts.get(etext, 0):\n",
        "            entity[\"ambiguous\"] = True\n",
        "\n",
        "        # Regex to find only full-word (bounded) occurrences (hyphens and underscores are the only type of interpunctuation not seen as word-boundaries)\n",
        "        pattern = re.compile(rf'(?<![\\w\\-=\\u2011]){re.escape(etext)}(?![\\w\\-=\\u2011])')\n",
        "        for match in pattern.finditer(text):\n",
        "            start, end = match.span()\n",
        "            if all(end <= s or start >= e for s, e in used_spans):\n",
        "                entity[\"S\"] = start\n",
        "                entity[\"En\"] = end\n",
        "                used_spans.append((start, end))\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Entity {entity} not found in row:\", row)\n",
        "            entity[\"not_found\"] = True\n",
        "\n",
        "    return entities\n",
        "\n",
        "def find_entity_positions(df, text_col=\"text\", entity_col=\"entities\"):\n",
        "    df[entity_col] = df.apply(add_positions_and_flag_ambiguities, axis=1)\n",
        "    return df\n",
        "\n",
        "# Function for checking if entity start and end point align with token boundaries\n",
        "# Scans a DataFrame for misaligned entity spans and prints any issues\n",
        "\n",
        "def check_for_misaligned_entities(df, text_col=\"text\", entity_col=\"entities\"):\n",
        "    tokenizer = RegexpTokenizer(r'[\\w\\-=]+|[^\\w\\s]', flags=re.UNICODE)\n",
        "    all_correct = True\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        text = row.get(text_col)\n",
        "        entities = row.get(entity_col)\n",
        "\n",
        "        if not isinstance(text, str) or not isinstance(entities, list):\n",
        "            continue\n",
        "\n",
        "        # Get token offsets\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        token_offsets = []\n",
        "        offset = 0\n",
        "        for token in tokens:\n",
        "            start = text.find(token, offset)\n",
        "            end = start + len(token)\n",
        "            token_offsets.append((start, end))\n",
        "            offset = end\n",
        "\n",
        "        # Token boundary sets\n",
        "        boundary_starts = {start for start, _ in token_offsets}\n",
        "        boundary_ends = {end for _, end in token_offsets}\n",
        "\n",
        "        # Check each entity\n",
        "        misaligned = []\n",
        "        for entity in entities:\n",
        "            s = entity.get(\"S\")\n",
        "            e = entity.get(\"En\")\n",
        "            if not isinstance(s, int) or not isinstance(e, int):\n",
        "                misaligned.append(entity)\n",
        "                continue\n",
        "            if s not in boundary_starts or e not in boundary_ends:\n",
        "                misaligned.append(entity)\n",
        "\n",
        "        # Report if needed\n",
        "        if misaligned:\n",
        "            print(f\"\\n❌ Misaligned entities in row {idx}:\")\n",
        "            for ent in misaligned:\n",
        "                print(ent)\n",
        "            all_correct = False\n",
        "    if all_correct:\n",
        "      print(\"No misaligned entities found.\")\n",
        "\n",
        "# Functions for transforming texts + entities (collected in a df) to iob-format\n",
        "\n",
        "def tokenize_and_iob(text, span_dicts, overlap_tolerance=False):\n",
        "    \"\"\"\n",
        "    Tokenizes the text using nltk and converts entity spans to IOB format.\n",
        "\n",
        "    Args:\n",
        "        text (str): The full input text.\n",
        "        span_dicts (list of dict): Each dict contains 'S' (start), 'En' (end), 'T' (label).\n",
        "        overlap_tolerance (bool): If True, allows partial overlap between token and entity span.\n",
        "\n",
        "    Returns:\n",
        "        tokens (list of str): Tokenized text.\n",
        "        tags (list of str): Corresponding IOB tags.\n",
        "    \"\"\"\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tags = ['O'] * len(tokens)\n",
        "\n",
        "    # Get character offsets for each token\n",
        "    offset = 0\n",
        "    token_offsets = []\n",
        "    for token in tokens:\n",
        "        start = text.find(token, offset)\n",
        "        end = start + len(token)\n",
        "        token_offsets.append((start, end))\n",
        "        offset = end\n",
        "\n",
        "    # Assign IOB tags\n",
        "    for span in span_dicts:\n",
        "        ent_start = span['S']\n",
        "        ent_end = span['En']\n",
        "        label = span['T']\n",
        "        first_token = True\n",
        "\n",
        "        for i, (tok_start, tok_end) in enumerate(token_offsets):\n",
        "            if overlap_tolerance:\n",
        "                match = tok_end > ent_start and tok_start < ent_end  # any overlap\n",
        "            else:\n",
        "                match = tok_start >= ent_start and tok_end <= ent_end  # fully inside\n",
        "\n",
        "            if match:\n",
        "                prefix = 'B' if first_token else 'I'\n",
        "                tags[i] = f'{prefix}-{label}'\n",
        "                first_token = False\n",
        "\n",
        "    return tokens, tags\n",
        "\n",
        "def transform_to_iob(df, text_col=\"text\", entity_col=\"entities\", iob_col=\"iob\", overlap_tolerance=False):\n",
        "\n",
        "    def transform_row_to_iob(row):\n",
        "        text = row[text_col]\n",
        "        entities = row[entity_col]\n",
        "        if not isinstance(text, str) or not isinstance(entities, list):\n",
        "            return []\n",
        "        tokens, tags = tokenize_and_iob(text, entities, overlap_tolerance)\n",
        "        return tags\n",
        "\n",
        "    df[iob_col] = df.apply(transform_row_to_iob, axis=1)\n",
        "    return df\n",
        "\n",
        "# Function for changing from one row per entity to one row per text (entities packed in list of dictionaries)\n",
        "def transform_from_entity_to_text_level(df):\n",
        "  # Create the entities list for each row\n",
        "  df['entities'] = df.apply(lambda row: {\n",
        "      'E': row['entity'],\n",
        "      'T': row['label'],\n",
        "      'id': row['annotation_id'],\n",
        "      'S': row['start'],\n",
        "      'En': row['end']\n",
        "  }, axis=1)\n",
        "\n",
        "  # Group by 'document' and aggregate entity_dicts\n",
        "  df = df.groupby('document').agg({\n",
        "      'entities': list,\n",
        "      'text': 'first'  # Retain the text if needed\n",
        "  }).rename(columns={'entities': 'entities'}).reset_index()\n",
        "\n",
        "  return df\n",
        "\n",
        "# Function for visualising sequeval metrics per category/label in a heatmap\n",
        "def visualize_seqeval_report(report_text):\n",
        "    # Parse the classification report text\n",
        "    lines = report_text.strip().splitlines()\n",
        "    start_index = next(i for i, line in enumerate(lines) if 'precision' in line.lower())\n",
        "\n",
        "    label_lines = [\n",
        "        line for line in lines[start_index + 1:]\n",
        "        if not line.strip().startswith(('micro', 'macro', 'weighted')) and line.strip()\n",
        "    ]\n",
        "\n",
        "    data = []\n",
        "    for line in label_lines:\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) >= 4:\n",
        "            label = \" \".join(parts[:-4])\n",
        "            precision, recall, f1, support = parts[-4:]\n",
        "            data.append([label, float(precision), float(recall), float(f1), int(support)])\n",
        "\n",
        "    df = pd.DataFrame(data, columns=[\"Label\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n",
        "    df.set_index(\"Label\", inplace=True)\n",
        "\n",
        "    # Define the color map: red → yellow → green\n",
        "    cmap = sns.color_palette(\"RdYlGn\", as_cmap=True)\n",
        "\n",
        "    # Plot heatmap\n",
        "    plt.figure(figsize=(10, len(df) * 0.5 + 1))\n",
        "    ax = sns.heatmap(\n",
        "        df[[\"Precision\", \"Recall\", \"F1-Score\"]],\n",
        "        annot=True,\n",
        "        cmap=cmap,\n",
        "        fmt=\".2f\",\n",
        "        linewidths=0.5,\n",
        "        linecolor='white',\n",
        "        cbar_kws={\"label\": \"Score\"}\n",
        "    )\n",
        "\n",
        "    # Reverse the x-axis label position to the top\n",
        "    ax.xaxis.set_ticks_position('top')\n",
        "    ax.xaxis.set_label_position('top')\n",
        "    ax.set_ylabel(\"Entity Label\", fontsize=12)\n",
        "\n",
        "    # Rotate ticks\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0TURMNlMiQex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Upload and preprocessing**"
      ],
      "metadata": {
        "id": "PTUIGaIDfBj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a first step, the data must be loaded. One input text can either be constituted by a **single list or a single list item**. However, a consistent choice within the data set is important.\n",
        "\n",
        "Input texts can be either uploaded as **separate txt-files from a shared folder** ([Option 1](#scrollTo=0wirvBzB9yHT)) or from a **csv-file** ([Option 2](#scrollTo=H-WFwDcg-XVK)).\n",
        "\n",
        "Depending on the data, **optional preprocessing steps** are recommended: In case the texts contain line-breaks, additional spaces or other digitisation artefacts, **cleaning** (cf. [here](#scrollTo=2jSFoR-h_h7M)) is recommended, and in case the texts are relatively long,  comparing their length with current LLM length limits and/or **splitting** them into multiple parts (cf. [here](#scrollTo=fnmRugLfAQqn)) can be helpful before NER."
      ],
      "metadata": {
        "id": "YI6SLoQJfZOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 1: Upload from folder\n",
        "= Each input text is collected in a folder as an individual txt-file (encoding: UTF-8), clearly differentiated by its unique file name (used as `\"id\"`)."
      ],
      "metadata": {
        "id": "0wirvBzB9yHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/llm-assisted-list-analysis/example-data/input-texts/WD-arrival-lists\"  #Set to folder containing the text file❗️\n",
        "\n",
        "ids = []\n",
        "texts = []\n",
        "\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.txt'):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "        key = os.path.splitext(file_name)[0]\n",
        "        ids.append(key)\n",
        "        texts.append(content)\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({'id': ids, 'text': texts})\n",
        "data.head()"
      ],
      "metadata": {
        "id": "i7ZEsQds-_Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 2: Upload from csv\n",
        "\n",
        "= All input texts and their metadata are given in form of a csv-file, where the column **\"id\"** (string) contains an unique identifier and the column **\"text\"** (string) contains the text itself. Further columns with additional metadata are allowed."
      ],
      "metadata": {
        "id": "H-WFwDcg-XVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = \"/content/llm-assisted-list-analysis/example-data/input-texts/RD-arrival-list-entries.csv\" #Set to path of csv file❗️\n",
        "data = pd.read_csv(csv_path)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "s4L9O-NQkYgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Cleaning of full texts"
      ],
      "metadata": {
        "id": "2jSFoR-h_h7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function **clean()** takes care of the following **preprocessing steps**:\n",
        "*   Removal of empty lines\n",
        "*   Removal of line breaks\n",
        "*   Merging of words seperated by line break - handled case-sensitive:\n",
        "    *   hyphen is kept if upper case after line break (e.g. `\"Stadt=\\nTor\"` > `\"Stadt=Thor\"`)\n",
        "    *   hyphen is removed if lower case after line break (e.g. `\"be=\\ngleiten\"` > `\"begleiten\"`)\n",
        "* Normalization of hyphens:\n",
        "    * default setting: single (\"-\") and double hyphens (\"=\") are preserved (`normalize_hyphens=False`)\n",
        "    * for transforming all double hyphens into single hyphens: `normalize_hyphens=True`"
      ],
      "metadata": {
        "id": "c35Rw0icr7bN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking an exemplary look: Comparing original vs. cleaned text\n",
        "\n",
        "text = data.iloc[0]['text'] # Getting the first text of the DataFrame - change index number to see another text\n",
        "print(f\"Original text:\\n{text}\")\n",
        "cleaned_text = clean(text, normalize_hyphens=False) # normalize_hypens can be set to True or False\n",
        "print(f\"\\nCleaned text:\\n{cleaned_text}\")"
      ],
      "metadata": {
        "id": "mvGC09aTvx9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning all uploaded texts\n",
        "data['text'] = data['text'].apply(lambda x: clean(x, normalize_hyphens=\"no\"))\n",
        "data.head() # Preview of cleaned data"
      ],
      "metadata": {
        "id": "gW8oU2EqfD1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Checking and adapting text length"
      ],
      "metadata": {
        "id": "fnmRugLfAQqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each LLM is limited to a certain **maximum number of tokens that can be processed in a single request**, including both the input (prompt) and the output (generated text). Although these limits are getting larger and larger with new LLM generations, you might want to check if your texts' length does not exceed the limit of the model you want to use. Also, keep in mind that LLM tokens do not equal traditional white-space token counts. To understand the difference and learn about current limits, check out: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
        "\n",
        "You can also use the Tokenizer tool to measure the LLM token length of your input texts for a quick estimation: https://platform.openai.com/tokenizer (Note: Besides the input text itself, further needed tokens for prompt, e.g. examples, and output must also be taken into consideration.)"
      ],
      "metadata": {
        "id": "RwBhnsJvBmyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifying the longest text (in white-space tokens)\n",
        "longest_text = data.loc[data['text'].str.len().idxmax()]\n",
        "print(f\"Length of longest text (ID: {longest_text['id']}): {len(longest_text['text'].split())} white-space tokens\")\n",
        "print(f\"{longest_text['text']}\")"
      ],
      "metadata": {
        "id": "ItZ5Ml3UFRId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WIP: Using tiktoken for token estimation"
      ],
      "metadata": {
        "id": "UWyHsQTV2mgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WIP: Splitting texts with a certain length into multiple parts"
      ],
      "metadata": {
        "id": "Ow_JojCy2wCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Named Entity Recognition (NER)**\n",
        "\n",
        "Due to their differences to 'normal' news or texts (e.g. less syntax, higher entity density), lists often pose a challenge for non-generative pre-trained models, e.g. Early High German models on [Huggingface](https://huggingface.co/), which are usually trained on more prototypical data. Additionally, list entries often contain highly specific types of entities.\n",
        "\n",
        "In this Notebook, LLMs are used for **automatically extracting (!) named entities within lists** (for extraction vs. annotation of NEs, cf. [Rastinger 2024](https://agki-dh.github.io/pages/webinar/page-7.html)). Among other things, this approach is helpful for low-resource scenarios such as the analysis of specific types of lists, as it allows for high flexibility and eliminates the need for an extensive annotated training set. To be more concrete, the case study \"Visiting Vienna\" used the **OpenAI API model \"gpt-3.5-turbo\"** via the **Python Library [Promptify](https://github.com/promptslab/Promptify)** which is also used in this Notebook."
      ],
      "metadata": {
        "id": "73JWS_yQfEyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the LLM pipeline and choosing the model"
      ],
      "metadata": {
        "id": "F4XWtQ6LmSQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The default model chosen in Promptify is \"gpt-3.5-turbo\". Alternatively the following OpenAI models can be used by changing the value of the argument `\"model\"`:\n",
        "\n",
        "*   \"text-davinci-003\"\n",
        "* \"davinci\"\n",
        "* \"text-davinci-001\"\n",
        "* \"ada\"\n",
        "*   \"text-curie-001\"\n",
        "*   \"text-ada-001\"\n",
        "* \"text-babbage-001\"\n",
        "* \"curie\"\n",
        "* \"text-davinci-002\"\n",
        "* \"gpt-4-0314\"\n",
        "* \"gpt-3.5-turbo-16k-0613\"\n",
        "* \"gpt-3.5-turbo-0301\"\n",
        "* \"gpt-3.5-turbo-16k\"\n",
        "* \"gpt-4\"\n",
        "* \"gpt-3.5-turbo\"\n",
        "* \"gpt-3.5-turbo-0613\"\n",
        "* \"gpt-4-0613\"\n",
        "\n",
        "Note that availability is dependent on OpenAI and may be subject to change. Besides OpenAI models, models from [HuggingFace](https://huggingface.co/) can also be called via `HubModel()`."
      ],
      "metadata": {
        "id": "UcF6ClNWijcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = userdata.get('openai_api') # OpenAI API key needs to have been saved in Colab Secrets as \"openai_api\" plus access to this key needs to be granted for the Notebook ❗️\n",
        "model = OpenAI(api_key, model=\"gpt-3.5-turbo\") # Choose model ❗️\n",
        "prompter = Prompter('/content/llm-assisted-list-analysis/prompt-templates/prompt.jinja') # Adapt to path to your jinja-file if repo is not loaded❗️\n",
        "pipe = Pipeline(prompter, model)"
      ],
      "metadata": {
        "id": "9hqH8U9BjXa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting NER parameters"
      ],
      "metadata": {
        "id": "ey4AOF0X6Xyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Besides choosing the model, users can decide on many other aspects when using LLMs for NER, e.g. on defining (1) [annotation categories or labels](#scrollTo=zwZjXQktVaIQ), (2) [examples](#scrollTo=QBS3krOpRJO3), (3) [text domain](#scrollTo=tm1HoUKxTG0U) and various [other factors](#scrollTo=ZyjzZ0RMRiex). These parameters are explained in more details below where they can also be set if desired. (Note: All parameters are optional.)"
      ],
      "metadata": {
        "id": "upEpYNu1Tdsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Categories or Labels"
      ],
      "metadata": {
        "id": "zwZjXQktVaIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the main advantages of using LLMs for list analysis, instead of pre-trained Named Entity Recognizers, is the **free choice of labels**. Lists frequently follow a highly specific structure and thus benefit from the possibility of more **fine-grained entity types** (e.g. for arrival lists: \"place of destination\", \"place of accomodation\", \"city gate\" etc. instead of overarching \"place\"). The chosen labels are defined as a **list of strings**:\n"
      ],
      "metadata": {
        "id": "MfaT9cTB2w9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [\"Person\", \"Stadttor\", \"Datum\", \"Herkunftsort\", \"Unterkunftsort\", \"Zielort\"] # German Labels used in the Case Study - adapt accordingly ❗️"
      ],
      "metadata": {
        "id": "L9jNFULgVZYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Examples"
      ],
      "metadata": {
        "id": "QBS3krOpRJO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLMs can make use of exemplary input-output-pairs (so-called\n",
        "**\"In-Context-Learning\"**). Three main approaches are usually differentiated here:\n",
        "\n",
        "1.   **Zero-shot** = no examples are given\n",
        "2.   **One-shot** = one example is given\n",
        "3. **Few-shot** = two or more examples are given\n",
        "\n",
        "Based on my own experiences and a small literature-review (cf. [Rastinger 2024](https://agki-dh.github.io/pages/webinar/page-7.html)), it is **recommended to include examples**. To be used with this notebook, they must be given in the following format:\n",
        "`[input-text[{\"E\": entity1, \"T\": label1}, {\"E\": entity2, \"T\": label2}, ...]]`\n",
        "\n",
        "Depending on the desired result, the dictionary keys can be changed (e.g. `\"E\"` > `\"Entity\"`) and expanded freely, e.g. by additionally asking for start and end character (`\"S\"`, `\"En\"`). In this case, the `\"prompt.jinja\"` file must be changed accordingly. Also, note that **character positions generated by LLMs are usually hallucinated**, meaning they only constitute very rough estimates of the actual positions, and thus must be corrected during post-processing (cf. [here](#scrollTo=tHvejqWS_i7m)).\n",
        "\n",
        "In general, the most important thing is, again, **consistency**: The labels assigned to the examples should be present within the defined set of labels. Also, ideally, all labels are present within the examples.\n",
        "\n",
        "For the arrival lists of the *Wien[n]erisches Diarium*, two sets of examples (with and without character positions) are given in accompanying json-files (\"example_WD_with_positions.json\" and \"example_WD_without_positions.json\"). For easy experimentation with multiple example sets, own examples can either be added as json-files or hard-coded directly into the notebook:"
      ],
      "metadata": {
        "id": "QPHappfKRfoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use WD examples without positions\n",
        "with open(\"/content/llm-assisted-list-analysis/example-data/in-context-learning/WD_arrival_lists_without_positions.json\", \"r\", encoding=\"utf-8\") as file:\n",
        "    examples = json.load(file)"
      ],
      "metadata": {
        "id": "aZboTO6qKeXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use WD examples with positions\n",
        "with open(\"/content/llm-assisted-list-analysis/example-data/in-context-learning/WD_arrival_lists_with_positions.json\", \"r\", encoding=\"utf-8\") as file:\n",
        "    examples = json.load(file)"
      ],
      "metadata": {
        "id": "wz1a2-kC0G5S",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload own examples\n",
        "with open(\"example-path\", \"r\", encoding=\"utf-8\") as file: # Replace with path to json file ❗️\n",
        "    examples = json.load(file)"
      ],
      "metadata": {
        "id": "Yys4YNt30Q-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hard-code own examples\n",
        "examples = [\"This example is an example text.\",[{\"E\": \"example\", \"T\": \"example\"}, {\"E\": \"example\", \"T\": \"example\"}]] # Replace with actual examples ❗️"
      ],
      "metadata": {
        "id": "_tSR9Xeb0fFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Domain"
      ],
      "metadata": {
        "id": "tm1HoUKxTG0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although not used in the case study, the domain of the input texts can be described freely (e.g. `\"medical\"`, `\"news\"`, `\"literature\"`) by setting the `domain` parameter:"
      ],
      "metadata": {
        "id": "fT8mj74jss2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "domain = \"news\" # Add text domain here ❗️"
      ],
      "metadata": {
        "id": "8z_hP9wxVS6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Further parameters"
      ],
      "metadata": {
        "id": "ZyjzZ0RMRiex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the `prompt.jinja` file, Promptify builds the prompt based on your set of parameters. Further adaptions are thus possible by directly changing this file. For instance, by default, **role prompting** is employed (`\"You are a highly intelligent and accurate Named Entity Recognition (NER) system.\"`), but can optionally be avoided by deleting this sentence in the `prompt.jinja` file. For more general information on **Prompt Engineering**, cf. [Pollin 2024](https://agki-dh.github.io/pages/webinar/page-3.html) and [Prompt-Engineering-Guide](https://github.com/dair-ai/Prompt-Engineering-Guide).\n",
        "\n",
        "Besides adapting the prompt, further arguments can be set when setting up the LLM pipeline, e.g. `temperature` or `top_p`. For more details, cf. [Promptify](https://github.com/promptslab/Promptify/blob/main/promptify/models/text2text/api/openai_models.py) and [OpenAI Docs](https://platform.openai.com/docs/api-reference/responses/create).\n"
      ],
      "metadata": {
        "id": "l9qBuPAXTQ9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Named Entity Recognition - on individual lists"
      ],
      "metadata": {
        "id": "vqMgZfsofJhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To quickly gain first insights of actual results, the following cells allow for an overview of the currently set parameters and running NER on single selected texts."
      ],
      "metadata": {
        "id": "D7se8hXh6GeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Getting an overview of currently chosen parameters\n",
        "\n",
        "for var_name in ['labels', 'examples', 'description', 'domain']:\n",
        "    if var_name in locals() and locals()[var_name] != [] and locals()[var_name] != None:\n",
        "        print(f\"{var_name.capitalize()} defined: {locals()[var_name]}\")\n",
        "    else:\n",
        "        print(f\"No {var_name.capitalize()} defined\")"
      ],
      "metadata": {
        "id": "5PgwdmE7UkKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running NER on individual texts\n",
        "\n",
        "# By default, the first text [0] is chosen - change index to choose another text\n",
        "text = data.iloc[0]['text']\n",
        "\n",
        "result = ner(text,\n",
        "             labels=labels,\n",
        "             examples=examples,\n",
        "             domain=domain\n",
        "             )\n",
        "\n",
        "# Taking a look at the results\n",
        "print(\"\\n\\n\" + textwrap.fill(text, width=100) + \"\\n\") # keeps text from running over the edge\n",
        "for entity in result:\n",
        "  print(entity)"
      ],
      "metadata": {
        "id": "8QGnEPsKfM2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: By setting the `usage_statistics` argument to `True`, you can additionally get insights into certain **usage statistics**: (1) number of input tokens (\"prompt tokens\"), (2) number of output tokens (\"completion tokens\") and (3) total number of tokens needed."
      ],
      "metadata": {
        "id": "PdZYnhpZBmRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = data.iloc[0]['text']\n",
        "\n",
        "result, prompt_tokens, completion_tokens, total_tokens = ner(text,\n",
        "                                                            labels=labels,\n",
        "                                                            examples=examples,\n",
        "                                                            domain=domain,\n",
        "                                                            usage_statistics = True # here is the change\n",
        "                                                            )\n",
        "\n",
        "print(\"\\n\\n\" + textwrap.fill(text, width=100) + \"\\n\")\n",
        "for entity in result:\n",
        "  print(entity)\n",
        "print(f\"\\nPrompt tokens: {prompt_tokens}\")\n",
        "print(f\"Completion tokens: {completion_tokens}\")\n",
        "print(f\"Total tokens: {total_tokens}\")"
      ],
      "metadata": {
        "id": "6-gmkMjX4XZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Named Entity Recognition - on a larger batch of lists"
      ],
      "metadata": {
        "id": "0R8WqZAho1IJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose outputs and statistics you'd like to get for each input text - default: all chosen\n",
        "cols = [\"entities\", \"prompt_tokens\", \"completion_tokens\", \"total_tokens\"] # Delete unwanted columns ❗️\n",
        "\n",
        "# Ensuring columns for each output exist in the dataframe\n",
        "for col in cols:\n",
        "    if col not in data.columns:\n",
        "        data[col] = None\n",
        "\n",
        "for idx in data.index:\n",
        "  text = data.at[idx, \"text\"]\n",
        "\n",
        "  result, prompt_tokens, completion_tokens, total_tokens = ner(\n",
        "            text,\n",
        "            labels=labels,\n",
        "            examples=examples,\n",
        "            domain=domain,\n",
        "            usage_statistics=True) # Important: keep True!\n",
        "\n",
        "  # Saving only selected outputs\n",
        "  if \"entities\" in cols:\n",
        "    data.at[idx, \"entities\"] = result\n",
        "  if \"prompt_tokens\" in cols:\n",
        "    data.at[idx, \"prompt_tokens\"] = prompt_tokens\n",
        "  if \"completion_tokens\" in cols:\n",
        "    data.at[idx, \"completion_tokens\"] = completion_tokens\n",
        "  if \"total_tokens\" in cols:\n",
        "    data.at[idx, \"total_tokens\"] = total_tokens\n",
        "\n",
        "  # Saving after each row\n",
        "  data.to_csv(\"ner_output_progress.csv\", index=False)"
      ],
      "metadata": {
        "id": "ipdPTJZlpBap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving and downloading the results\n",
        "\n",
        "data.to_csv('ner_results.csv', index=False)\n",
        "files.download('ner_results.csv')"
      ],
      "metadata": {
        "id": "e7Yuf3gl9Fhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taking a look at the results"
      ],
      "metadata": {
        "id": "ZgxDFb3VbGuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking a look at the results\n",
        "data"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aLWU3lyQvlMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistics on results"
      ],
      "metadata": {
        "id": "ptNeBJlIp8rU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize totals\n",
        "total_entities = 0\n",
        "label_counts = {}\n",
        "\n",
        "# Iterate over rows and count labels\n",
        "for index, row in data.iterrows():\n",
        "  entities = row['entities']\n",
        "  if not entities or not isinstance(entities, list):\n",
        "    continue  # Skip if empty or not a list\n",
        "  total_entities += len(entities)\n",
        "  for entity in entities:\n",
        "    label = entity.get('T', 'Unknown')  # Use 'Unknown' if label is missing\n",
        "    label_counts[label] = label_counts.get(label, 0) + 1\n",
        "\n",
        "# Prepare results table\n",
        "table_data = [\n",
        "    (label, count, f\"{(count / total_entities) * 100:.2f}%\")\n",
        "    for label, count in sorted(label_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "]\n",
        "\n",
        "# Print statistics\n",
        "print(f\"Total entities identified: {total_entities}\")\n",
        "print(tabulate(table_data, headers=[\"Label\", \"Count\", \"Percentage\"], tablefmt=\"fancy_grid\"))"
      ],
      "metadata": {
        "id": "sZKmS-up2SXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistics on usage"
      ],
      "metadata": {
        "id": "ah4zC_fSqC73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Token columns\n",
        "usage_cols = [\"prompt_tokens\", \"completion_tokens\"]\n",
        "\n",
        "# Calculate total from total_tokens column\n",
        "total_tokens = data[\"total_tokens\"].sum()\n",
        "\n",
        "# Sum prompt and completion tokens separately\n",
        "token_totals = {col: data[col].sum() for col in usage_cols}\n",
        "\n",
        "# Prepare table with percentage of total_tokens\n",
        "token_table = [\n",
        "    (col, token_totals[col], f\"{(token_totals[col] / total_tokens) * 100:.2f} %\")\n",
        "    for col in usage_cols\n",
        "]\n",
        "\n",
        "# Print the results\n",
        "print(f\"Total Tokens needed: {total_tokens}\")\n",
        "print(tabulate(token_table, headers=[\"Token Type\", \"Total\", \"Percentage\"], tablefmt=\"fancy_grid\"))"
      ],
      "metadata": {
        "id": "_TIefjg2qCsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Postprocessing**"
      ],
      "metadata": {
        "id": "giOm1iAg5-Be"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrating entity IDs"
      ],
      "metadata": {
        "id": "GeN7gM8hCAFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For safer post-processing, each generated entity is given a **unique ID** (`\"id\"`). Among other things, this can later be used to easily correct errors and differentiate between linguistically identical entities.\n"
      ],
      "metadata": {
        "id": "a0LpufO_CGad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = add_unique_entity_ids(data)\n",
        "\n",
        "# Taking a look at the difference in the first entity of the first text\n",
        "print(data.loc[0, \"entities\"][0])"
      ],
      "metadata": {
        "id": "LPy2AgjRB-VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identifying hallucinations"
      ],
      "metadata": {
        "id": "yhkQP8VX73cW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLMs can **hallucinate**, meaning they may produce results that were not in the input. For NER, this can apply to both entities and labels and may vary from fully hallucinated results (e.g. not-defined label) to single-character differences (e.g. normalised writing), meaning certain hallucinations may - depending on the project goals - still be considered 'correct'. To allow for easy overview and integration, the following cells check for all divergences between in- and output and mark them through **designated columns** (`\"hallucinated_labels\"`, `\"hallucinated_entities\"`). Additionally, **statistics on hallucinations** can be generated."
      ],
      "metadata": {
        "id": "oIz45pqKq_Dx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hallucinated labels\n",
        "(= labels not in the defined label set)"
      ],
      "metadata": {
        "id": "GqZ7AYFh0q0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifying hallucinated labels - only applicable if a set of labels was defined\n",
        "\n",
        "# Function to check if all entity labels are in the label set\n",
        "def check_entity_labels(entity_list, labels):\n",
        "    if not entity_list:  # Skips unprocessed lists or empty outputs\n",
        "        return []\n",
        "    return [entity for entity in entity_list if entity.get(\"T\") not in labels] # Returns list of invalid entities (empty = all valid)\n",
        "\n",
        "# Apply to DataFrame\n",
        "data[\"hallucinated_labels\"] = data[\"entities\"].apply(lambda ents: check_entity_labels(ents, labels))\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "GxDQVJwiq5lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating statistics on hallucinated labels\n",
        "\n",
        "# Count total number of predicted entities\n",
        "total_entities = data[\"entities\"].dropna().apply(len).sum()\n",
        "\n",
        "# Count total number of hallucinated labels\n",
        "total_hallucinated = data[\"hallucinated_labels\"].dropna().apply(len).sum()\n",
        "\n",
        "# Calculate hallucination percentage\n",
        "percent_hallucinated = (total_hallucinated / total_entities * 100) if total_entities > 0 else 0\n",
        "\n",
        "hallucinated_labels_ov = [\n",
        "    [\"Hallucinated labels\", total_hallucinated],\n",
        "    [\"Total entities\", total_entities],\n",
        "    [\"Percentage of hallucinated labels\", f\"{percent_hallucinated:.2f} %\"]\n",
        "]\n",
        "\n",
        "print(tabulate(hallucinated_labels_ov, headers=[\"Metric\", \"Value\"], tablefmt=\"fancy_grid\"))"
      ],
      "metadata": {
        "id": "cZfaHLC90oNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hallucinated entities\n",
        "(= entities not included in exactly the same way in the input text; entities found more often than they appear in the input text are also counted as hallucinations)"
      ],
      "metadata": {
        "id": "RNaEQ0aG0vjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifying hallucinated entities\n",
        "\n",
        "def check_entity_texts(entity_list, source_text):\n",
        "    if not entity_list or not isinstance(entity_list, list) or not isinstance(source_text, str):\n",
        "        return []\n",
        "\n",
        "    # Extract predicted entity texts\n",
        "    predicted_texts = [entity.get(\"E\") for entity in entity_list if isinstance(entity, dict) and \"E\" in entity]\n",
        "    predicted_counts = Counter(predicted_texts)\n",
        "\n",
        "    # Count actual occurrences in the source text\n",
        "    actual_counts = Counter()\n",
        "    for text in predicted_texts:\n",
        "        actual_counts[text] = source_text.count(text)\n",
        "\n",
        "    # Identify hallucinated entities\n",
        "    hallucinated_entities = []\n",
        "    temp_counts = actual_counts.copy()\n",
        "    for entity in entity_list:\n",
        "        etext = entity.get(\"E\")\n",
        "        if etext is None:\n",
        "            continue\n",
        "        if temp_counts[etext] > 0:\n",
        "            temp_counts[etext] -= 1\n",
        "        else:\n",
        "            hallucinated_entities.append(entity)\n",
        "\n",
        "    return hallucinated_entities\n",
        "\n",
        "# Apply to DataFrame\n",
        "data[\"hallucinated_entities\"] = data.apply(\n",
        "    lambda row: check_entity_texts(row[\"entities\"], row[\"text\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "07VDk4cB73tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistics on hallucinated entities\n",
        "\n",
        "# Count total number of predicted entities\n",
        "total_entities = data[\"entities\"].dropna().apply(len).sum()\n",
        "\n",
        "# Count total number of hallucinated entities\n",
        "total_hallucinated_entities = data[\"hallucinated_entities\"].dropna().apply(len).sum()\n",
        "\n",
        "# Calculate hallucination percentage\n",
        "percent_hallucinated_entities = (\n",
        "    (total_hallucinated_entities / total_entities * 100) if total_entities > 0 else 0\n",
        ")\n",
        "\n",
        "# Prepare results\n",
        "hallucinated_entities_ov = [\n",
        "    [\"Hallucinated entities\", total_hallucinated_entities],\n",
        "    [\"Total entities\", total_entities],\n",
        "    [\"Percentage of hallucinated entities\", f\"{percent_hallucinated_entities:.2f}%\"]\n",
        "]\n",
        "\n",
        "# Display as table\n",
        "print(tabulate(hallucinated_entities_ov, headers=[\"Metric\", \"Value\"], tablefmt=\"fancy_grid\"))"
      ],
      "metadata": {
        "id": "x4WyqiXtBRaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identifying positions of entities"
      ],
      "metadata": {
        "id": "tHvejqWS_i7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the **correct positions of NEs within input texts** (`\"S\"` = start character, `\"En\"` = end character) is valuable for annotating, visualising, verifying  and evaluating them. As mentioned, LLMs tend to hallucinate when asked for character start and end points - which is why this information needs to be added afterwards by finding generated entities with the input text. (Note: At this step of the workflow, i.e. without manual corrections or fuzzy matching, hallucinated entities can not be found in the input text. This will be solved in a later step.)\n",
        "\n",
        "Tip: In case you asked the LLM for entity positions, you can also use the [display function](#scrollTo=Ha0Hrh1xlW-e) to take a look at possible non-alignments with actual entities.\n",
        "\n",
        "**Ambigous positions:** What poses a challenge are the relatively rare cases in which certain words/NEs are appearing multiple times in an input text, but are only identified as an entity once. This can, for instance, occur, if whole lists that tend to repeatedly contain the same entities are used as input text. Here, it may be unclear which mention was actually identified. Currently, the Notebook marks such entities with multiple possible positions with `\"ambiguous\"=True`.\n",
        "\n",
        "WIP: Based on the case study, so far, the only (approximation to a) solution for this problem seems to be taking into account (a) the order in which an LLM returns the entities as this usually corresponds to their in-text order and/or (b) the LLM-estimated positions. The code for these approaches is currently being updated and will be added to the Notebook in the future."
      ],
      "metadata": {
        "id": "U3b6wDnYBjDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding start (\"S\") and end (\"En\") to each entity\n",
        "data = find_entity_positions(data)\n",
        "\n",
        "# Taking a look at the difference in the first entity of the first text\n",
        "print(data.loc[0, \"entities\"][0])"
      ],
      "metadata": {
        "id": "fKEVBYhtvc1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting entities with \"ambiguous\"=True\n",
        "ambiguous_count = 0\n",
        "for index, row in data.iterrows():\n",
        "    entities = row['entities'] # Assuming entities are stored as strings\n",
        "    for entity in entities:\n",
        "        if isinstance(entity, dict) and entity.get('ambiguous', False) == True:\n",
        "            ambiguous_count += 1\n",
        "\n",
        "print(f\"Number of entities with ambiguous positions: {ambiguous_count}\")"
      ],
      "metadata": {
        "id": "vfYQlmCtCA1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WIP: optional: calculating statistics on difference between LLM-estimated and actual positions - in case the LLM was asked for entity positions as well"
      ],
      "metadata": {
        "id": "BYQ7PByBBFFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WIP: optional: item separation based on entity types (e.g. certain type of entity marks end point of an item) - in case whole lists are used as input texts"
      ],
      "metadata": {
        "id": "2TyhHxOwKcW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Displaying results**"
      ],
      "metadata": {
        "id": "Ha0Hrh1xlW-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Color palette - add more colors if more than six labels are used\n",
        "colors = ['#F0D9EF', '#FCDCE1', '#FFE6BB', '#E9ECCE', '#CDE9DC', '#C4DFE5']\n",
        "\n",
        "# Mapping each label from the label set to a color\n",
        "# Labels not in the defined label set will be shown in grey\n",
        "label_to_color = {label: colors[i % len(colors)] for i, label in enumerate(labels)}\n",
        "\n",
        "for _, row in data.head(5).iterrows(): # showing first five examples by default - change if needed or download all below\n",
        "    display(highlight_entities(row))\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "n5of6k5FyBqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading a HTML will all texts and their highlighted entities\n",
        "save_highlighted_entities_to_html(data, filename=\"annotated_entities.html\")"
      ],
      "metadata": {
        "id": "YiJLvnZtJH_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "Qqe-f3mHrWEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For optional (but recommended!) evaluation of the NER results, the following cells allow for **comparison with a goldstandard data set**, prepared outside of the Notebook with [CATMA](https://catma.de/) or another annotation platform."
      ],
      "metadata": {
        "id": "TkcMkmcp-SWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 1: Uploading goldstandard created with CATMA"
      ],
      "metadata": {
        "id": "TMrSZOXd9xqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One option for creating a goldstandard data set is [CATMA (= Computer Assisted Text Markup and Analysis)](https://catma.de/) where users can (collaboratively) annotate texts according to their individual tag sets. To be used in this Notebook, the annotations of selected documents can be downloaded from within CATMA as a zip-folder, extracted (= unzipped) and, lastly, uploaded here.\n",
        "\n",
        "To ensure compatability of NER results and goldstandard, the following things must be given:\n",
        "- identical full texts (e.g. both cleaned)\n",
        "- identical label set (i.e. defined in prompt vs. used for annotation)"
      ],
      "metadata": {
        "id": "_jMoJzAyHOg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_catma = \"/content/llm-assisted-list-analysis/example-data/goldstandard/RD-catma-export\" # Change to path to export results ❗️\n",
        "annotation_collection = \"NER\" # Change to name of CATMA annotation collection ❗️\n",
        "\n",
        "goldstandard = get_catma_annotations(folder_catma, annotation_collection)\n",
        "goldstandard = transform_from_entity_to_text_level(goldstandard)\n",
        "goldstandard.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Hn-ae0_xC9DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 2: Uploading goldstandard from csv-file"
      ],
      "metadata": {
        "id": "sBFHe7Yh990Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, the goldstandard can also be uploaded from a prepared csv-file, optimally in the same format as the NER output data. (Direct upload in IOB-format would theoretically also be feasible, however it is only recommended if the same tokenizer was used, since this would otherwise lead to misalignment.)"
      ],
      "metadata": {
        "id": "GOo4PD__ovUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "goldstandard = pd.read_csv(\"goldstandard.csv\") # Change to path to goldstandard as csv-file ❗️\n",
        "goldstandard.head()"
      ],
      "metadata": {
        "id": "KNHvKSHdso8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformating NER output and goldstandard to IOB format"
      ],
      "metadata": {
        "id": "CglGMs6kZL4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, tokenisation (here: custom - each token ...; e.g. ...), then transformation to IOB-Format (IOB2); RegEx Tokenizer: möglichst einheitliche Behandlung von Abkürzungen etc., möglichst feingliedrig - jedes Satzzeichen (auch Abkürzungspunkte, ausgenommen Hyphen) als Token\n",
        "\n",
        "As mentioned before, LLMs tokenize differently - meaning they might also have followed other rules when extracting entities. e.g. misalignment:\n",
        "\n",
        "by Default: not total overlap > ignored (= not found)\n",
        "change setting --> overlapping allowed"
      ],
      "metadata": {
        "id": "QQM9qhIVG0Np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up a costum tokenizer\n",
        "tokenizer = RegexpTokenizer(r'[\\w\\-=]+|[^\\w\\s]', flags=re.UNICODE)"
      ],
      "metadata": {
        "id": "AOMIL9qzLrWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Taking a look at how the tokenizer tokenizes\n",
        "text = \"Den 9. per ord. Postwagen nach Laber, Hr. Tirant, Hochfürstl. Taxischer Zahnarzt.\"\n",
        "print(tokenizer.tokenize(text))"
      ],
      "metadata": {
        "id": "-HKYz0tCGr4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for entities in the data whose start and end points misalign with token boundaries\n",
        "check_for_misaligned_entities(data)"
      ],
      "metadata": {
        "id": "XTGHI01ABN3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_for_misaligned_entities(goldstandard)"
      ],
      "metadata": {
        "id": "l0RwmGGA5Jqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Taking a look at how the data is transformed to IOB format\n",
        "text = \"Den 9. per ord. Postwagen nach Laber, Hr. Tirant, Hochfürstl. Taxischer Zahnarzt.\"\n",
        "span_dicts = [{'E': '9.', 'T': 'Datum', 'id': 'e1', 'S': 4, 'En': 6},{'E': 'Laber', 'T': 'Zielort', 'id': 'e2', 'S': 31, 'En': 36},{'E': 'Hr. Tirant', 'T': 'Person', 'id': 'e3', 'S': 38, 'En': 48},{'E': 'Hochfürstl. Taxischer Zahnarzt', 'T': 'Person', 'id': 'e4', 'S': 50, 'En': 80}]\n",
        "tokens, tags = tokenize_and_iob(text, span_dicts)\n",
        "print(tokens)\n",
        "print(tags)"
      ],
      "metadata": {
        "id": "llpI4JL0gxX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming data and goldstandard to IOB - added in form of a new column \"iob\"\n",
        "data = transform_to_iob(data)\n",
        "goldstandard = transform_to_iob(goldstandard)\n",
        "\n",
        "# must be sorted according to the same id\n",
        "true = goldstandard.sort_values(by=\"document\")[\"iob\"].tolist() #list of iob-lists sorted by \"document\" - adapt accordingly ❗️\n",
        "predicted = data.sort_values(by=\"paragraph_id\")[\"iob\"].tolist() #list of iob-lists sorted by \"paragraph_id\" - adapt accordingly ❗️\n",
        "\n",
        "# Taking a look at the IOB outputs in comparison to each other\n",
        "print(\"True (IOB):\", true)\n",
        "print(\"Predicted (IOB):\", predicted)"
      ],
      "metadata": {
        "id": "kZG5Bsj1-Bxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating evaluation metrics"
      ],
      "metadata": {
        "id": "Zh_WPxuR-7a0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more details on metrics for evaluation used here, see https://github.com/chakki-works/seqeval"
      ],
      "metadata": {
        "id": "4xm7Gcg46KsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting overall metrics\n",
        "accuracy = round(accuracy_score(true, predicted),2) # all values rounded to two decimal places\n",
        "precision = round(precision_score(true, predicted),2)\n",
        "recall = round(recall_score(true, predicted),2)\n",
        "f1 = round(f1_score(true, predicted),2)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1) #micro average used by default (cf. classification report below)"
      ],
      "metadata": {
        "id": "j_EUT1h__dN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting overview of metrics per category/label\n",
        "report = classification_report(true, predicted, digits=2)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "gu81fh3T2_4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_seqeval_report(report)"
      ],
      "metadata": {
        "id": "RRVRnkgy8FFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further planned features (work in progress)"
      ],
      "metadata": {
        "id": "R35NLOzK_DhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helping functions for manual error corrections (easy documentation of errors in excel file > automatic correction in results)"
      ],
      "metadata": {
        "id": "T3oES07PuixZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WIP: fast-runs of multiple building block combinations (e.g. import+cleaning+NER+download)"
      ],
      "metadata": {
        "id": "NZ-IfMV2uRZm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}